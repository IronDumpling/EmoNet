{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demonstration_based_on_OpenCV.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Facial recognition\n","\n","Some parts of the code is referencing https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/\n","\n","C. Combs, A. Rosebrock, Vipulnath, Tango, Beibeizeng, S. One, Wildan, S. Sarkar, Youwenjing, Palbha, C. K. Colita, Ratman, Ratman, Kk, C. Observer, Karen, TetsFR, Matt, Swapnil, S. Cox, Phillity, Ray, N. Phan, Usman, Han, GabriellaK, Mark, B. Benc, D. Ravarro, C. Burns, A. Varsha, D. Pritham, Peshmerge, Amal, E. n., Saverio, Cedric, Adanalı, K. Khatak, Gopalakrishna, Lii, K. Goyal, Luan, M. Rathna, T. Sokhib, Abhilash, Chopin, C. Wu, M. Faucheux, Ryan, Lizheng, Len, Ajithkumar, A. N. O'Nyme, Yunui, A. Woods, Abdulkadir, Mat, V. Gupta, Vasilis, A. Hormati, Ayush, Raunak, Pierre, M. Sharma, David, Dan, Murat, A. Mishra, Huzefa, Huzefa, E. Nguyen, Ranindrastia, Frances, Kondor, M. Tokat, Srinivasan, Eric, Rohan, R. Liston, Shohruh, R. Karabed, D. Poster, Jing, M, Shashank, S. Singh, Maduabuchi, A. Nawaz, B. Imran, Diego, Prashant, Vamshi, M. M. H, Pragati, Yan, Pratap, Xerebro, Jasen, Sherry, Eero, I. Vaid, Shubhayu, Nishant, R. Sarvaiya, Raksha, Jim, Ivan, Rencita, Nilay, L. Kruger, J. Q., Andreas, E. C. Nugroho, V. Mishra, DavidQ, N. A. Tuan, Vipul, D. Wells, Hartmut, V. Borana, J. T. Draper, R. Cannock, Dilip, M. Chawala, Sanjay, I. Saleh, Vinod, Andy, Dexter, Bay, Paolo, B. O, Hello, A. Tiwari, Sudheer, B. Michaelson, Agribot, Peter, P. Maurel, Supriya, Calista, M. Ovais, R. Gomez, Rajs, Louis, Kedar, Rishi, A. Belard, Prashant, K. Inam, J. Gun, A. Datir, Mario, A. K. Panigrahi, A. Aneja, Sparrow, A. Odo, R. joshi, Michael, N. Kashyap, J. Jordan, Brendhow, Brightstar, Gagan, D. Desai, Hassan, V. Kulkarni, Z. Li, Avinandan, L. Hsing, M. L., D. Gupta, M. A. Qadir, tuna_hsp, A. Ozcan, Nikhil, Rohit, LGenzelis, P. Verma, Charlie, Harsath, P. Bansal, V. Sati, K. T. Hyeon, Clarke, Sachin, Hongmin, A. Mannan, Sapa, A. Mishra, Plamen, Sampath, Archie, Joel, Jhony, Chris, P. Unruh, Jeffery, Abid, Yash, H. Maddela, Kanishk, Pranav, K. Maricq, Sakshi, D. Hoffman, A. garg, Tn, Mpho, M. Wang, T. Yuanlai, Yusra, M. Cain, Naomi, I. Sukheja, Alvaro, Raghu, Michelle, and Python, “Face detection with opencv and Deep Learning,” PyImageSearch, 04-Jul-2021. [Online]. Available: https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/. [Accessed: 12-Apr-2022]. \n","\n","\n","Python libraries are as the follows:"],"metadata":{"id":"_najjpCwWjyM"}},{"cell_type":"code","source":["import torchvision.models\n","import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","alexnet = torchvision.models.alexnet(pretrained=True)"],"metadata":{"id":"NlnePfQFz0oj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import os\n","import numpy as np\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","\n","import torch \n","import torch.nn as nn \n","import torch.nn.functional as F \n","import torch.optim as optim "],"metadata":{"id":"kFokey5-zzLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJx7GWXzWd1r"},"outputs":[],"source":["import imutils\n","import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import math\n","import matplotlib.pyplot as plt\n","\n","from skimage import io\n","from skimage.transform import resize\n","import os\n","\n","fixed_size = [224, 48]\n","use_cuda = True"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e2joTMAldGQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651081116,"user_tz":240,"elapsed":725,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"dd835440-b11f-4809-b6db-296231057a43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"pSb3BnYX_atl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c714c82-aa6e-4ce6-d40f-f421f4039304","executionInfo":{"status":"ok","timestamp":1649651081545,"user_tz":240,"elapsed":431,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}}},"source":["!wget -N https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n","!wget -N https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-11 04:24:41--  https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 28104 (27K) [text/plain]\n","Saving to: ‘deploy.prototxt’\n","\n","\rdeploy.prototxt       0%[                    ]       0  --.-KB/s               \rdeploy.prototxt     100%[===================>]  27.45K  --.-KB/s    in 0.002s  \n","\n","Last-modified header missing -- time-stamps turned off.\n","2022-04-11 04:24:41 (16.5 MB/s) - ‘deploy.prototxt’ saved [28104/28104]\n","\n","--2022-04-11 04:24:41--  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10666211 (10M) [application/octet-stream]\n","Saving to: ‘res10_300x300_ssd_iter_140000.caffemodel’\n","\n","res10_300x300_ssd_i 100%[===================>]  10.17M  --.-KB/s    in 0.07s   \n","\n","Last-modified header missing -- time-stamps turned off.\n","2022-04-11 04:24:41 (137 MB/s) - ‘res10_300x300_ssd_iter_140000.caffemodel’ saved [10666211/10666211]\n","\n"]}]},{"cell_type":"markdown","source":["Can choose two ways to upload images: either upload an existing image from your library, or take an photo with the webcam.\n","\n","The following code is the block for utilizing the webcam."],"metadata":{"id":"rqBsxBeKZKDS"}},{"cell_type":"code","source":["# code for using the web cam is from the same reference\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"metadata":{"id":"LsWJNFBkkqwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for testing purpose\n","# path can be changed to any image if you want\n","image_file = \"/content/e975082a339596293eb55a5ea5a0f57.jpg\"\n","# image_file = take_photo()\n","# print(image_file)"],"metadata":{"id":"Gia0JKOkZg99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# image = cv2.imread(image_file, cv2.IMREAD_UNCHANGED)\n","image = cv2.imread(image_file)\n","\n","# resize it to have a maximum width of 400 pixels\n","image = imutils.resize(image, width=400)\n","(h, w) = image.shape[:2]\n","print(w,h)\n","cv2_imshow(image)"],"metadata":{"id":"b7wi3kCXW24G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmECWHDlEfSq"},"source":["Download the pre-trained face detection model, consisting of two files:\n","\n","- The network definition (deploy.prototxt)\n","- The learned weights (res10_300x300_ssd_iter_140000.caffemodel) "]},{"cell_type":"code","metadata":{"id":"NHPblHBY8UfU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31054234-fd38-484b-ebb1-0a8337f226e7","executionInfo":{"status":"ok","timestamp":1649651487868,"user_tz":240,"elapsed":219,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}}},"source":["print(\"[INFO] loading model...\")\n","prototxt = 'deploy.prototxt'\n","model = 'res10_300x300_ssd_iter_140000.caffemodel'\n","net = cv2.dnn.readNetFromCaffe(prototxt, model)\n","\n","# resize it to have a maximum width of 400 pixels\n","image = imutils.resize(image, width=400)\n","blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] loading model...\n"]}]},{"cell_type":"code","metadata":{"id":"kF3EhuvS5j53","colab":{"base_uri":"https://localhost:8080/"},"outputId":"519e7cf5-3816-4596-ef76-c4090e48f4c3","executionInfo":{"status":"ok","timestamp":1649651488681,"user_tz":240,"elapsed":127,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}}},"source":["print(\"[INFO] computing object detections...\")\n","net.setInput(blob)\n","detections = net.forward()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] computing object detections...\n"]}]},{"cell_type":"markdown","metadata":{"id":"DfYUKtIA6KBe"},"source":["Loop over the detections and crop detected faces."]},{"cell_type":"code","metadata":{"id":"AozMWEAb6Kr7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"211b73ed-f95b-4ee2-abbf-44db8f290756","executionInfo":{"status":"ok","timestamp":1649651489512,"user_tz":240,"elapsed":135,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}}},"source":["list = []\n","for i in range(0, detections.shape[2]):\n","\n","\t# extract the confidence (i.e., probability) associated with the prediction\n","\tconfidence = detections[0, 0, i, 2]\n","\n","\t# filter out weak detections by ensuring the `confidence` is\n","\t# greater than the minimum confidence threshold\n","\tif confidence > 0.5:\n","\t\t# compute the (x, y)-coordinates of the bounding box for the object\n","\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\t\n","\t\t# print(\"startX, startY, endX, endY, w, h: \", startX, startY, endX, endY, w, h)\n","\t\t# by printing the x, y coordinates, finds out that it draws a rectangle,\n","\t\t# needs to convert to square coordinates\n","\t\tlength_x = endX - startX\n","\t\tlength_y = endY - startY\n","\n","\t\t# make sure that no faces are out of bounds\n","\t\tif length_x > length_y:\n","\t\t\tstartY -= (length_x - length_y) / 2\n","\t\t\tendY += (length_x - length_y) / 2\n","\t\t\tif startY < 0:\n","\t\t\t\tstartY = 0\n","\t\t\t\tendY = length_x\n","\t\t\telif endY > h:\n","\t\t\t\tendY = h\n","\t\t\t\tstartY = h - length_x\n","\t\telse:\n","\t\t\tstartX -= math.floor(length_y - length_x) / 2\n","\t\t\tendX += math.floor(length_y - length_x) / 2\n","\t\t\tif startX < 0:\n","\t\t\t\tstartX = 0\n","\t\t\t\tendX = length_y\n","\t\t\telif endY > w:\n","\t\t\t\tendX = w\n","\t\t\t\tstartX = w - length_y\n","\t\tstartX, endX = math.floor(startX), math.ceil(endX)\n","\t\tstartY, endY = math.floor(startY), math.ceil(endY)\n","\n","\t\t# store in a list so that we can have multiple images of faces if presented in one image\n","\t\tprint(\"startX, startY, endX, endY: \", startX, startY, endX, endY)\n","\t\tlist.append([startX, startY, endX, endY])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["startX, startY, endX, endY:  96 82 306 291\n"]}]},{"cell_type":"code","source":["cropped_img = []\n","if list:\n","  for i in range(len(list)):\n","    (startX, startY, endX, endY) = list[i]\n","    img = image[startY : endY, startX : endX]\n","    cv2_imshow(img)\n","    img = imutils.resize(img, width = fixed_size[1])\n","    cropped_img.append(img)"],"metadata":{"id":"zDCrBXJJq_p5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hh0b0Jtq7pa9"},"source":["Show the resulting image and save to the demo directory."]},{"cell_type":"code","metadata":{"id":"5WICWY6_7p6b"},"source":["cv2_imshow(image)\n","for j in range(len(cropped_img)):\n","  cv2_imshow(cropped_img[j])\n","  cv2.imwrite('/content/drive/MyDrive/APS360/Project_dataset/Demo/demo/cropped_img_' + str(j) + '.jpg', cropped_img[j])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Visualise Result\n","\n","Original Image and corresponding emoji"],"metadata":{"id":"SWY9wR1wq4KP"}},{"cell_type":"markdown","source":["## VGG or ResNet demo\n"],"metadata":{"id":"rZv7uWqbhD8r"}},{"cell_type":"markdown","source":["Import the pre-trained ResNet from my teammates, use the ResNet with the correct parameters to test the output of different photos."],"metadata":{"id":"GMWEF3ByTwjs"}},{"cell_type":"code","source":["import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","from torch.nn.modules import dropout\n","from torch.nn.modules.activation import ReLU"],"metadata":{"id":"lDpGaEBUHdMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet_model_1 = models.resnet.resnet18(pretrained=True)\n","num_ftrs = resnet_model_1.fc.in_features\n","resnet_model_1.fc = nn.Linear(num_ftrs, 6)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","resnet_model_1 = resnet_model_1.to(device)"],"metadata":{"id":"Q6h51MRRls0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best model for ResNet or VGG\n","if use_cuda and torch.cuda.is_available():\n","  resnet_model_1.cuda()\n","  print('CUDA is available!  Training on GPU ...')\n","\n","best_model_path1 = '/content/model_ResNet_0'\n","\n","# Load saved best model\n","resnet_model_1.load_state_dict(torch.load(best_model_path1))"],"metadata":{"id":"HZ7GIUK36rHj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651496323,"user_tz":240,"elapsed":168,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"13febb30-2682-4062-8284-a736bcacc1e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available!  Training on GPU ...\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["Here are some transformations and data-loader."],"metadata":{"id":"s6mziczQT9Pk"}},{"cell_type":"code","source":["# when you have the best model, do transformations to the demo image based on the model chosen\n","\n","im_transforms = transforms.Compose([transforms.ToTensor()])"],"metadata":{"id":"kXlYJZIj75-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# demo image has been stored in the demo directory\n","# for the presentation purpose, there will be only one image each time\n","demon_dir = '/content/drive/MyDrive/APS360/Project_dataset/Demo'\n","# change this !\n","demo_dataset = datasets.ImageFolder(demon_dir, transform=im_transforms)\n","demo_loader = torch.utils.data.DataLoader(demo_dataset, batch_size=1)"],"metadata":{"id":"PSMkUqi2Fw1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# test"],"metadata":{"id":"AzNCctOT1rxR"}},{"cell_type":"code","source":["classes = ['disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"],"metadata":{"id":"863kFbfSeHKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following codes shows the steps on how to correctly input the testing image to the model and produces the prediction."],"metadata":{"id":"OEQ4KBgJUFBP"}},{"cell_type":"code","source":["# load the best model and train the inputs\n","# obtain one batch of training images\n","dataiter = iter(demo_loader)\n","images, labels = dataiter.next()\n","inputs = images.cuda()\n","outputs = resnet_model_1(inputs) # compute output like in\n","print(outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651500097,"user_tz":240,"elapsed":4,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"6533bd10-3419-4021-fd7a-ccff43bee678","id":"HyWQSoPWYDf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.5552, -4.1699, -2.5938, -1.6245, -3.4276, -4.3075]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["outputs_avg = outputs.view(1, -1).mean(0)\n","print(outputs_avg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651500097,"user_tz":240,"elapsed":4,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"3c973d07-fb76-4b3b-91d2-bfec9e3f9162","id":"DqSBb6tsYDf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-1.5552, -4.1699, -2.5938, -1.6245, -3.4276, -4.3075], device='cuda:0',\n","       grad_fn=<MeanBackward1>)\n"]}]},{"cell_type":"code","source":["# pred.append(F.softmax(output).tolist())\n","score = F.softmax(outputs_avg)\n","print(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sek_lyzgZVNS","executionInfo":{"status":"ok","timestamp":1649651500098,"user_tz":240,"elapsed":4,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"dbfaadd0-cc28-4342-b30a-5d192af987e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.3879, 0.0284, 0.1373, 0.3620, 0.0596, 0.0247], device='cuda:0',\n","       grad_fn=<SoftmaxBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  \n"]}]},{"cell_type":"code","source":["_, predicted = torch.max(outputs_avg.data, 0)\n","print(predicted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651500098,"user_tz":240,"elapsed":4,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"e94b1198-bf50-4f44-ee44-a224b8bd1600","id":"A1GV83A0YDf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0, device='cuda:0')\n"]}]},{"cell_type":"code","source":["ind = int(predicted.cpu().numpy())\n","# print(ind)\n","# print(classes[2])\n","pred_class = str(classes[ind])\n","print(pred_class)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649651500098,"user_tz":240,"elapsed":3,"user":{"displayName":"Haoran Yin","userId":"17167352899892743994"}},"outputId":"14c5f22d-b8d9-4037-913a-0679c0b53a54","id":"CJ8Gx78PYDf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["disgust\n"]}]},{"cell_type":"code","source":["# Below codes copied from \n","raw_img = io.imread('/content/drive/MyDrive/APS360/Project_dataset/Demo/demo/cropped_img_0.jpg')\n","axes=plt.subplot(1, 3, 1)\n","plt.imshow(raw_img)\n","plt.xlabel('Input Image', fontsize=16)\n","axes.set_xticks([])\n","axes.set_yticks([])\n","plt.tight_layout()\n","\n","plt.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.9, hspace=0.02, wspace=0.3)\n","plt.subplot(1, 3, 2)\n","ind = 0.1+0.6*np.arange(len(classes))    # the x locations for the groups\n","width = 0.4       # the width of the bars: can also be len(x) sequence\n","color_list = ['red','orangered','darkorange','limegreen','darkgreen','royalblue','navy']\n","for i in range(len(classes)):\n","    plt.bar(ind[i], score.data.cpu().numpy()[i], width, color=color_list[i])\n","plt.title(\"Classification results \",fontsize=20)\n","plt.xlabel(\" Expression Category \",fontsize=16)\n","plt.ylabel(\" Classification Score \",fontsize=16)\n","plt.xticks(ind, classes, rotation=90, fontsize=14)\n","\n","axes=plt.subplot(1, 3, 3)\n","emojis_img = io.imread('/content/drive/MyDrive/APS360/Project_dataset/images/emoji/%s.png' % pred_class)\n","plt.imshow(emojis_img)\n","plt.xlabel('Emoji Expression', fontsize=16)\n","axes.set_xticks([])\n","axes.set_yticks([])\n","plt.tight_layout()\n","# # show emojis\n","\n","plt.show()\n","plt.close()\n","\n","print(\"The Expression is %s\" %pred_class)"],"metadata":{"id":"6TU4QXjCFw1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AlexNet demo"],"metadata":{"id":"j9Dq4y7UhAZA"}},{"cell_type":"markdown","source":["AlexNet demo is here for the purpose of testing the demonstration code. The final model chosen is not AlexNet, however. But the procedures in this section is the same as the one above."],"metadata":{"id":"UGb_-ZBlUOrB"}},{"cell_type":"code","source":["# when you have the best model, do transformations to the demo image based on the model chosen\n","\n","im_transforms = transforms.Compose([transforms.Resize(fixed_size), transforms.ToTensor(), transforms.Grayscale(num_output_channels=3)])\n","labels = ['happy', 'surprise', 'sad', 'neutral', 'fear', 'disgust', 'angry']"],"metadata":{"id":"kjzEySMk3hWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# demo image has been stored in the demo directory\n","# for the presentation purpose, there will be only one image each time\n","demon_dir = '/content/drive/MyDrive/APS360/Project_dataset/Demo'\n","# change this !\n","demo_dataset = datasets.ImageFolder(demon_dir, transform=im_transforms)\n","demo_loader = torch.utils.data.DataLoader(demo_dataset, batch_size=1)"],"metadata":{"id":"6YiBrW_o3bT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# AlexNet Architecture \n","# Version 1.0\n","class AlexClassifier(nn.Module):\n","  def __init__(self):\n","    super(AlexClassifier, self).__init__()\n","    self.name = 'AlexClassifier'\n","    # looking for a three-channel input\n","    self.conv1 = nn.Conv2d(256, 2048, 3)\n","    self.pool = nn.MaxPool2d(2, 2)\n","    self.fc1 = nn.Linear(2 * 2 * 2048, 2048)\n","    self.fc2 = nn.Linear(2048, 1024)\n","    self.fc3 = nn.Linear(1024, 256)\n","    self.fc4 = nn.Linear(256, 7)\n","\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = x.view(-1, 2 * 2 * 2048) #flatten feature data\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = F.relu(self.fc3(x))\n","    x = self.fc4(x)\n","    return x"],"metadata":{"id":"K-IIOWAHzuMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best model\n","best_model = AlexClassifier()\n","if use_cuda and torch.cuda.is_available():\n","  best_model.cuda()\n","  print('CUDA is available!  Training on GPU ...')\n","\n","best_model_path = '/content/model_{0}_bs{1}_lr{2}_epoch{3}'.format('AlexClassifier', 32, 3e-5, 13)\n","\n","# Load saved best model\n","best_model.load_state_dict(torch.load(best_model_path))"],"metadata":{"id":"lA_P20yEhJCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Demonstrate the primary AlexNet model \n","alex_demon_dir = '/content/drive/MyDrive/APS360/Project_dataset/alex_data/demo/'\n","alex_demon_loader = torch.utils.data.DataLoader(demo_dataset, batch_size=1)\n","\n","# Compute features for the demonstration images \n","n = 0 \n","for img, label in iter(alex_demon_loader): \n","  if use_cuda and torch.cuda.is_available():\n","    features = alexnet.features(img).cuda()\n","    features_tensor = torch.from_numpy(features.cpu().detach().numpy())\n","\n","  folder_dir = alex_demon_dir + str('happy')\n","  if not os.path.isdir(folder_dir):\n","    os.mkdir(folder_dir)\n","  torch.save(features_tensor.squeeze(0), folder_dir + '/' + str(n) + '.tensor')\n","  n += 1\n","\n","# Reload the computed features \n","alex_demon_features = torchvision.datasets.DatasetFolder(alex_demon_dir, loader=torch.load, extensions=('.tensor'))\n","alex_demon_features_loader = torch.utils.data.DataLoader(alex_demon_features, batch_size=32)"],"metadata":{"id":"eF130NBGAaOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = [] \n","for img, label in iter(alex_demon_loader): \n","  if use_cuda and torch.cuda.is_available():\n","    features = alexnet.features(img).cuda()\n","    print(features.shape)\n","    features_tensor = torch.from_numpy(features.cpu().detach().numpy())\n","    # features_tensor = features_tensor.squeeze(0)\n","    print(features_tensor.cuda().shape)\n","output = best_model(features_tensor.cuda())\n","print(output)\n","# pred.append(F.softmax(output).tolist())\n","softmax = F.softmax(output).tolist()\n","print(softmax)"],"metadata":{"id":"R5T0YpXPDPqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs_avg = output.view(1, -1).mean(0)\n","print(outputs_avg)"],"metadata":{"id":"o7YeAjmzXZs-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, predicted = torch.max(outputs_avg.data, 0)\n","print(predicted)"],"metadata":{"id":"efzEfBo4Xflk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(int(predicted.cpu().numpy()))\n","# print(labels[5])\n","pred_class = str(labels[int(predicted.cpu().numpy())])\n","print(pred_class)"],"metadata":{"id":"nkBg_m22Xnxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["axes=plt.subplot(1, 3, 3)\n","emojis_img = io.imread('/content/drive/MyDrive/APS360/Project_dataset/images/emoji/%s.png' % str(labels[int(predicted.cpu().numpy())]))\n","plt.imshow(emojis_img)\n","plt.xlabel('Emoji Expression', fontsize=16)\n","axes.set_xticks([])\n","axes.set_yticks([])\n","plt.tight_layout()"],"metadata":{"id":"m1VcAK7wZT7F"},"execution_count":null,"outputs":[]}]}